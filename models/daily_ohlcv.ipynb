{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "from utils.timescale_connector import TimescaleConnector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TimescaleConnector.query_ohlcv_daily(\"ACB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for a time series dataset\n",
    "# First test only with \"close\"\n",
    "data = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to between -1 and 1, this matches the range of the tanh activation function\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookback period\n",
    "lookback = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to Torch tensor\n",
    "data = torch.FloatTensor(data).reshape(-1, lookback, len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "def create_inout_sequences(input_data: pd.DataFrame, tw: int) -> list:\n",
    "    inout_seq = []\n",
    "    length = len(input_data)\n",
    "    for i in range(length-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sequence\n",
    "inout_seq = create_inout_sequences(data, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inout_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, input_size=5, hidden_layer_size=5, output_size=5):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, len(df.columns)), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]  \n",
    "    \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        return super().training_step(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model, define loss function and optimzier\n",
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49minout_seq)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:937\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    935\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 937\u001b[0m _verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    939\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:37\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[0;32m---> 37\u001b[0m     __verify_train_val_loop_configuration(trainer, model)\n\u001b[1;32m     38\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[1;32m     39\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mVALIDATING:\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:57\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     55\u001b[0m has_training_step \u001b[39m=\u001b[39m is_overridden(\u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_training_step:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m has_optimizers \u001b[39m=\u001b[39m is_overridden(\u001b[39m\"\u001b[39m\u001b[39mconfigure_optimizers\u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_optimizers:\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=1000, log_every_n_steps=2, accelerator=\"gpu\")\n",
    "trainer.fit(model=model, train_dataloaders=inout_seq, training_ste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (\n",
    "            torch.zeros(1, 1, model.hidden_layer_size),\n",
    "            torch.zeros(1, 1, model.hidden_layer_size),\n",
    "        )\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i%25 == 1:\n",
    "        print(f\"Epoch {i:3} loss: {single_loss.item():10.8f}\")\n",
    "        \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Set your desired batch size\n",
    "dataloader = DataLoader(inout_seq, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 1, got 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_loop(dataloader, model, loss_function, optimizer)\n",
      "\u001b[1;32m/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_seq):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(input_seq\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(input_seq) ,\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_cell)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(lstm_out\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(input_seq), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tb24/projects/long-term-stock-recommendation-system/models/daily_ohlcv.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/rnn.py:875\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    872\u001b[0m             hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m    873\u001b[0m         \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    874\u001b[0m         \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/rnn.py:790\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    786\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    787\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    788\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    789\u001b[0m                        ):\n\u001b[0;32m--> 790\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    791\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    792\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    794\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/stocks/lib/python3.11/site-packages/torch/nn/modules/rnn.py:236\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{\u001b[39;00mexpected_input_dim\u001b[39m}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 1, got 10"
     ]
    }
   ],
   "source": [
    "train_loop(dataloader, model, loss_function, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
